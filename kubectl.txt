==========================================================================================================================================================

# How to add roles to nodes in Kubernetes?



Default:

root@instance-20240602-125323:~# kubectl get nodes
NAME                       STATUS   ROLES           AGE   VERSION
instance-20240602-125323   Ready    control-plane   39m   v1.29.0
instance-20240602-125903   Ready    <none>          38m   v1.29.0
instance-20240602-125931   Ready    <none>          38m   v1.29.0


Add Role
kubectl label node <node name> node-role.kubernetes.io/<role name>=<key - (any name)>



kubectl label node instance-20240602-125931 node-role.kubernetes.io/worker-1=w1

root@instance-20240602-125323:~# kubectl get nodes
NAME                       STATUS   ROLES           AGE   VERSION
instance-20240602-125323   Ready    control-plane   42m   v1.29.0
instance-20240602-125903   Ready    <none>          42m   v1.29.0
instance-20240602-125931   Ready    worker-1        42m   v1.29.0


kubectl label node instance-20240602-125903 node-role.kubernetes.io/worker-2=w2

root@instance-20240602-125323:~# kubectl get nodes
NAME                       STATUS   ROLES           AGE   VERSION
instance-20240602-125323   Ready    control-plane   43m   v1.29.0
instance-20240602-125903   Ready    worker-2        42m   v1.29.0
instance-20240602-125931   Ready    worker-1        42m   v1.29.0

Remove Role

kubectl label node <node name> node-role.kubernetes.io/<role name>-


kubectl label node instance-20240602-125931 node-role.kubernetes.io/worker-1-


node/instance-20240602-125931 unlabeled
root@instance-20240602-125323:~# kubectl get nodes
NAME                       STATUS   ROLES           AGE   VERSION
instance-20240602-125323   Ready    control-plane   52m   v1.29.0
instance-20240602-125903   Ready    worker-2        52m   v1.29.0
instance-20240602-125931   Ready    <none>          52m   v1.29.0


kubectl label node instance-20240602-125903 node-role.kubernetes.io/worker-2-
node/instance-20240602-125903 unlabeled
root@instance-20240602-125323:~# kubectl get nodes
NAME                       STATUS   ROLES           AGE   VERSION
instance-20240602-125323   Ready    control-plane   53m   v1.29.0
instance-20240602-125903   Ready    <none>          52m   v1.29.0
instance-20240602-125931   Ready    <none>          52m   v1.29.0





============================================================
# Kubernetes API - Get Pods on Specific Nodes




kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=instance-20240602-125931


kubectl get pods -n kube-system -o wide --field-selector spec.nodeName=instance-20240602-125323

kubectl get pods -n kube-system -o wide --field-selector spec.nodeName=instance-20240602-125931

kubectl get pods -n kube-system -o wide --field-selector spec.nodeName=instance-20240602-125903


root@instance-20240602-125323:~# kubectl get pods -n kube-system -o wide --field-selector spec.nodeName=instance-20240602-125323
NAME                                               READY   STATUS    RESTARTS   AGE   IP            NODE                       NOMINATED NODE   READINESS GATES
calico-kube-controllers-658d97c59c-lnctm           1/1     Running   0          67m   10.85.0.2     instance-20240602-125323   <none>           <none>
calico-node-dlvrv                                  1/1     Running   0          67m   10.128.0.12   instance-20240602-125323   <none>           <none>
coredns-76f75df574-l7r6v                           1/1     Running   0          67m   10.85.0.4     instance-20240602-125323   <none>           <none>
coredns-76f75df574-qzg98                           1/1     Running   0          67m   10.85.0.3     instance-20240602-125323   <none>           <none>
etcd-instance-20240602-125323                      1/1     Running   0          68m   10.128.0.12   instance-20240602-125323   <none>           <none>
kube-apiserver-instance-20240602-125323            1/1     Running   0          68m   10.128.0.12   instance-20240602-125323   <none>           <none>
kube-controller-manager-instance-20240602-125323   1/1     Running   0          68m   10.128.0.12   instance-20240602-125323   <none>           <none>
kube-proxy-lgkmd                                   1/1     Running   0          67m   10.128.0.12   instance-20240602-125323   <none>           <none>
kube-scheduler-instance-20240602-125323            1/1     Running   0          68m   10.128.0.12   instance-20240602-125323   <none>           <none>
root@instance-20240602-125323:~# kubectl get pods -n kube-system -o wide --field-selector spec.nodeName=instance-20240602-125931
NAME                READY   STATUS    RESTARTS   AGE   IP            NODE                       NOMINATED NODE   READINESS GATES
calico-node-2hhpm   1/1     Running   0          67m   10.128.0.14   instance-20240602-125931   <none>           <none>
kube-proxy-5jlv5    1/1     Running   0          67m   10.128.0.14   instance-20240602-125931   <none>           <none>
root@instance-20240602-125323:~# kubectl get pods -n kube-system -o wide --field-selector spec.nodeName=instance-20240602-125903
NAME                READY   STATUS    RESTARTS   AGE   IP            NODE                       NOMINATED NODE   READINESS GATES
calico-node-tk4m4   1/1     Running   0          68m   10.128.0.13   instance-20240602-125903   <none>           <none>
kube-proxy-bd6sl    1/1     Running   0          68m   10.128.0.13   instance-20240602-125903   <none>           <none>

============================================================================================================================================================

# How can I keep a container running on Kubernetes?


However, I will describe both the ways(Considering you are running busybox container):




In order to keep a POD running it should to be performing certain task, otherwise Kubernetes will find it unnecessary, therefore it stops. There are many ways to keep a POD running.


Running sleep command while running the container.

Running an infinite loop inside the container.
Although the first option is easier than the second one and may suffice the requirement, it is not the best option. As, there is a limit as far as the number of seconds you are going to assign in the sleep command. But a container with infinite loop running inside it never exits.

However, I will describe both the ways(Considering you are running busybox container):

1. Sleep Command

apiVersion: v1
kind: Pod
metadata:
  name: busybox
  labels:
    app: busybox
spec:
  containers:
  - name: busybox
    image: busybox
    ports:
    - containerPort: 80
    command: ["/bin/sh", "-ec", "sleep 1000"]



2. Infinite Loop

apiVersion: v1
kind: Pod
metadata:
  name: busybox
  labels:
    app: busybox
spec:
  containers:
  - name: busybox
    image: busybox
    ports:
    - containerPort: 80
    command: ["/bin/sh", "-ec", "while :; do echo '.'; sleep 5 ; done"]
====================================================================================================================
# kubectl describe node instance-20240602-125323




Name:               instance-20240602-125323
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=instance-20240602-125323
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 10.128.0.12/32
                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.99.0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 02 Jun 2024 13:05:51 +0000
Taints:             node-role.kubernetes.io/control-plane:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  instance-20240602-125323
  AcquireTime:     <unset>
  RenewTime:       Sun, 02 Jun 2024 14:23:30 +0000
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Sun, 02 Jun 2024 13:06:32 +0000   Sun, 02 Jun 2024 13:06:32 +0000   CalicoIsUp                   Calico is running on this no   de
  MemoryPressure       False   Sun, 02 Jun 2024 14:23:31 +0000   Sun, 02 Jun 2024 13:05:48 +0000   KubeletHasSufficientMemory   kubelet has sufficient memor   y available
  DiskPressure         False   Sun, 02 Jun 2024 14:23:31 +0000   Sun, 02 Jun 2024 13:05:48 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Sun, 02 Jun 2024 14:23:31 +0000   Sun, 02 Jun 2024 13:05:48 +0000   KubeletHasSufficientPID      kubelet has sufficient PID a   vailable
  Ready                True    Sun, 02 Jun 2024 14:23:31 +0000   Sun, 02 Jun 2024 13:05:55 +0000   KubeletReady                 kubelet is posting ready sta   tus. AppArmor enabled
Addresses:
  InternalIP:  10.128.0.12
  Hostname:    instance-20240602-125323
Capacity:
  cpu:                2
  ephemeral-storage:  10089736Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             4023012Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  9298700683
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3920612Ki
  pods:               110
System Info:
  Machine ID:                 bf3ab7a0a48fccb5f4eb2b6ff6154ae2
  System UUID:                bf3ab7a0-a48f-ccb5-f4eb-2b6ff6154ae2
  Boot ID:                    e91408ea-0c14-4840-a5a1-d7826f33700c
  Kernel Version:             6.1.0-21-cloud-amd64
  OS Image:                   Debian GNU/Linux 12 (bookworm)
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  cri-o://1.28.4
  Kubelet Version:            v1.29.0
  Kube-Proxy Version:         v1.29.0
PodCIDR:                      192.168.0.0/24
PodCIDRs:                     192.168.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                                ------------  ----------  ---------------  -------------  ---
  kube-system                 calico-kube-controllers-658d97c59c-lnctm            0 (0%)        0 (0%)      0 (0%)           0 (0%)         77m
  kube-system                 calico-node-dlvrv                                   250m (12%)    0 (0%)      0 (0%)           0 (0%)         77m
  kube-system                 coredns-76f75df574-l7r6v                            100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     77m
  kube-system                 coredns-76f75df574-qzg98                            100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     77m
  kube-system                 etcd-instance-20240602-125323                       100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         77m
  kube-system                 kube-apiserver-instance-20240602-125323             250m (12%)    0 (0%)      0 (0%)           0 (0%)         77m
  kube-system                 kube-controller-manager-instance-20240602-125323    200m (10%)    0 (0%)      0 (0%)           0 (0%)         77m
  kube-system                 kube-proxy-lgkmd                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         77m
  kube-system                 kube-scheduler-instance-20240602-125323             100m (5%)     0 (0%)      0 (0%)           0 (0%)         77m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1100m (55%)  0 (0%)
  memory             240Mi (6%)   340Mi (8%)
  ephemeral-storage  0 (0%)       0 (0%)
  hugepages-1Gi      0 (0%)       0 (0%)
  hugepages-2Mi      0 (0%)       0 (0%)
Events:              <none>


====================================================================================================================================================

# watch -n .5 curl 10.106.122.78:8080

=========================================================================================================================================================













